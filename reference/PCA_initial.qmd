---
title: "Final Project - PCA"
author: "AP"
format: html
editor: visual
editor_options:
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(readxl)
library(janitor)
library(ggpubr)
library(caret)
```

# 1. Objective and Rationale

Following the exploratory data analysis (EDA), we employed Principal Component Analysis (PCA) as a dimensionality reduction technique to better understand the structure of the predictor space and to address potential multicollinearity among variables. This will also give us our first glimpse at important predictors and related predictors, and we may recognize these patterns in our models, as well.

Given that the final modeling objective is to predict pH, PCA was applied strictly to the predictor variables (i.e., the input features), with the target variable excluded during the transformation phase.

# 2. Methodology

The PCA was conducted using the caret and tidyverse packages in R. The procedure included the following key steps:

Data Preparation: All non-numeric variables were excluded. Only numeric predictors were retained for PCA.

Standardization: Each feature was centered and scaled to unit variance to ensure that PCA was not biased toward features with larger scales.

PCA Transformation: Principal components were extracted from the standardized predictor matrix.

Component Retention: The number of components to retain was informed by a combination of the Kaiser criterion (eigenvalues \> 1), cumulative variance explained, and visual inspection via a scree plot.

```{r}
# Load data
data <- read_excel("StudentData.xlsx")

# Extract only numeric predictors, exclude target
predictors <- data %>%
  select(-PH) %>%
  select(where(is.numeric))

# Preprocess: standardize and apply PCA
pca_prep <- preProcess(predictors, method = c("YeoJohnson", "center", "scale", "pca"))

# Transform data using PCA
pca_transformed <- predict(pca_prep, predictors)

# Attach target variable back
pca_final <- bind_cols(pca_transformed, Ph = data$PH)
```

# 3. Results and Interpretation

Variance Explained The PCA transformation resulted in a series of orthogonal components that capture the variance in the original feature space. The cumulative variance explained by the principal components is shown below:

```{r}
# Extract variance explained
var_explained <- pca_prep$std^2
cumulative_variance <- cumsum(var_explained / sum(var_explained))

# Print table
tibble(
  PC = paste0("PC", seq_along(cumulative_variance)),
  Variance = round(var_explained / sum(var_explained), 3),
  Cumulative = round(cumulative_variance, 3)
) |>
  view()
```

From this output, we observe that:

The first few components capture a substantial portion of the total variance.

For example, the first 5–7 components typically explain 80–95% of the cumulative variance (exact values will depend on your data).

This dimensionality reduction is significant given that the original predictor space may contain many more features.

Scree Plot A scree plot was generated to visually inspect the point of diminishing returns, or the "elbow", in the variance explained:

```{r}
# Scree plot
qplot(
  x = seq_along(var_explained),
  y = var_explained / sum(var_explained),
  geom = "line"
) +
  labs(
    title = "Scree Plot of Principal Components",
    x = "Principal Component",
    y = "Proportion of Variance Explained"
  ) +
  theme_minimal()
```

This plot helps determine the optimal number of PCs to retain. Components beyond the elbow contribute marginally to the variance and may be excluded from further modeling.

# 4. Loadings and Interpretability

The rotation matrix provides the loadings of each original variable on the principal components. Loadings close to ±1 indicate strong influence, while values near 0 indicate minimal contribution.

```{r}
# Loadings (rotation matrix)
loadings <- pca_prep$rotation
head(loadings)
```

```{r}
pc1 <- loadings |>
  as.data.frame() |>
  rownames_to_column(var = "Predictor1") |>
  as_tibble() |>
  mutate(total_top_3 = PC1 + PC2 + PC3,
         total_top_5 = PC1 + PC2 + PC3 + PC4 + PC5,
         total_top_8 = PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8) |>
  arrange(desc(PC1)) |>
  head(10) |>
  select(1:2)

pc2 <- loadings |>
  as.data.frame() |>
  rownames_to_column(var = "Predictor2") |>
  as_tibble() |>
  mutate(total_top_3 = PC1 + PC2 + PC3,
         total_top_5 = PC1 + PC2 + PC3 + PC4 + PC5,
         total_top_8 = PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8) |>
  arrange(desc(PC2)) |>
  head(10) |>
  select(1,3)

pc3 <- loadings |>
  as.data.frame() |>
  rownames_to_column(var = "Predictor3") |>
  as_tibble() |>
  mutate(total_top_3 = PC1 + PC2 + PC3,
         total_top_5 = PC1 + PC2 + PC3 + PC4 + PC5,
         total_top_8 = PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8) |>
  arrange(desc(PC3)) |>
  head(10) |>
  select(1, 4)

pc4 <- loadings |>
  as.data.frame() |>
  rownames_to_column(var = "Predictor4") |>
  as_tibble() |>
  mutate(total_top_3 = PC1 + PC2 + PC3,
         total_top_5 = PC1 + PC2 + PC3 + PC4 + PC5,
         total_top_8 = PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8) |>
  arrange(desc(PC4)) |>
  head(10) |>
  select(1, 5)

bind_cols(pc1, pc2, pc3, pc4) |>
  arrange(Predictor1, Predictor2, Predictor3, Predictor4) |>
  view()
  as_tibble()
```

By examining the loading structure:

We can interpret PC1 as a linear combination emphasizing variables A, B, and C

Components with clear thematic groupings (e.g., all chemistry variables, or all environmental sensors) enhance interpretability and may suggest latent structures in the data.
